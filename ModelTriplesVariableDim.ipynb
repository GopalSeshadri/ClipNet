{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCHS = 12\n",
    "LEARNING_RATE = 0.00003\n",
    "MARGIN = 10\n",
    "CAPTION_INPUT_SIZE = 300\n",
    "FRAME_INPUT_SIZE = 500\n",
    "CAPTION_LATENT_SIZE = 256\n",
    "FRAME_LATENT_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating placeholders\n",
    "caption_placeholder = tf.placeholder(tf.float32, shape = [None, None, CAPTION_INPUT_SIZE])\n",
    "frame_1_placeholder = tf.placeholder(tf.float32, shape = [None, None, FRAME_INPUT_SIZE])\n",
    "frame_2_placeholder = tf.placeholder(tf.float32, shape = [None, None, FRAME_INPUT_SIZE])\n",
    "\n",
    "# Setting GPU config\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.8)\n",
    "config = tf.ConfigProto(allow_soft_placement = True, gpu_options = gpu_options)\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Neural Network Graph for modified Siamese Network\n",
    "def train_caption_embeddings(x_placeholder, latent_dim):\n",
    "    cell = tf.nn.rnn_cell.GRUCell(latent_dim, kernel_initializer = tf.contrib.layers.variance_scaling_initializer(), name = 'caption_cells')\n",
    "    cells = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob = 0.5)\n",
    "    x, s = tf.nn.dynamic_rnn(cells, x_placeholder, dtype = tf.float32, swap_memory = True)\n",
    "    x = tf.contrib.layers.batch_norm(x, is_training = True, updates_collections = None)\n",
    "    x = tf.nn.dropout(x, rate = 0.5)\n",
    "    print(x.shape)\n",
    "    x = tf.reshape(x, shape = [-1, 50 * latent_dim])\n",
    "    print(x.shape)\n",
    "    x = tf.nn.dropout(x, rate = 0.5)\n",
    "    x = tf.layers.dense(x, latent_dim, kernel_initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "    out = tf.nn.relu(x)\n",
    "    print(out.shape)\n",
    "    return out\n",
    "\n",
    "def train_frame_embeddings(x_placeholder, latent_dim, reuse):\n",
    "    with tf.compat.v1.variable_scope('var', reuse = reuse):\n",
    "        cell = tf.nn.rnn_cell.GRUCell(latent_dim, kernel_initializer = tf.contrib.layers.variance_scaling_initializer(), name = 'frame_cells')\n",
    "        cells = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob = 0.5)\n",
    "        x, s = tf.nn.dynamic_rnn(cells, x_placeholder, dtype = tf.float32, swap_memory = True)\n",
    "        x = tf.contrib.layers.batch_norm(x, is_training = True, updates_collections = None)\n",
    "        x = tf.nn.dropout(x, rate = 0.5)\n",
    "        print(x.shape)\n",
    "        x = tf.reshape(x, shape = [-1, 50 * latent_dim])\n",
    "        print(x.shape)\n",
    "        x = tf.nn.dropout(x, rate = 0.5)\n",
    "        x = tf.layers.dense(x, latent_dim, kernel_initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "        out = tf.nn.relu(x)\n",
    "        print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-4-4bd95ea02567>:2: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-4-4bd95ea02567>:4: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "(?, ?, 256)\n",
      "(?, 12800)\n",
      "WARNING:tensorflow:From <ipython-input-4-4bd95ea02567>:11: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "(?, 256)\n",
      "(?, ?, 300)\n",
      "(?, 15000)\n",
      "(?, 300)\n",
      "(?, ?, 300)\n",
      "(?, 15000)\n",
      "(?, 300)\n"
     ]
    }
   ],
   "source": [
    "caption_out = train_caption_embeddings(caption_placeholder, CAPTION_LATENT_SIZE) #Anchor\n",
    "frame_out_1_full = train_frame_embeddings(frame_1_placeholder, FRAME_LATENT_SIZE, reuse = None) #Positive\n",
    "frame_out_2_full = train_frame_embeddings(frame_2_placeholder, FRAME_LATENT_SIZE, reuse = True) #Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the first CAPTION_LATENT_SIZE from frame latent vector for positive and negatives\n",
    "frame_out_1 = frame_out_1_full[:, :CAPTION_LATENT_SIZE]\n",
    "frame_out_2 = frame_out_2_full[:, :CAPTION_LATENT_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Triplet loss\n",
    "positive_distance = tf.reduce_sum(tf.square(caption_out - frame_out_1), 1)\n",
    "negative_distance = tf.reduce_sum(tf.square(caption_out - frame_out_2), 1)\n",
    "\n",
    "loss = tf.reduce_mean(tf.maximum(0., positive_distance - negative_distance + MARGIN))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = LEARNING_RATE)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# init = tf.global_variables_initializer()\n",
    "# sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Triples\n",
    "with open(r'/home/login/Paired/train_triples_list.pickle', 'rb') as f:\n",
    "    train_triples_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'/home/login/Paired/val_triples_list.pickle', 'rb') as f:\n",
    "    val_triples_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_anchor = np.stack([each[0] for each in train_triples_list], axis = 0)\n",
    "train_positive = np.stack([each[1] for each in train_triples_list], axis = 0)\n",
    "train_negative = np.stack([each[2] for each in train_triples_list], axis = 0)\n",
    "\n",
    "val_anchor = np.stack([each[0] for each in val_triples_list], axis = 0)\n",
    "val_positive = np.stack([each[1] for each in val_triples_list], axis = 0)\n",
    "val_negative = np.stack([each[2] for each in val_triples_list], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(461750, 50, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_anchor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(461750, 50, 500)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_positive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(461750, 50, 500)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_negative.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461750"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192750, 50, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_anchor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192750, 50, 500)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_positive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192750, 50, 500)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_negative.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLLOOOO\n",
      "After epoch 0 train loss is 62994.8881 valid loss is 35435.9102\n",
      "HELLLOOOO\n",
      "After epoch 1 train loss is 28087.8364 valid loss is 28221.2196\n",
      "HELLLOOOO\n",
      "After epoch 2 train loss is 11335.5035 valid loss is 23442.9009\n",
      "HELLLOOOO\n",
      "After epoch 3 train loss is 5266.5304 valid loss is 21918.4990\n",
      "HELLLOOOO\n",
      "After epoch 4 train loss is 2560.8567 valid loss is 20187.3888\n",
      "HELLLOOOO\n",
      "After epoch 5 train loss is 1259.7281 valid loss is 19149.5896\n",
      "HELLLOOOO\n",
      "After epoch 6 train loss is 940.5610 valid loss is 20363.5976\n",
      "HELLLOOOO\n",
      "After epoch 7 train loss is 570.1578 valid loss is 19685.7288\n",
      "HELLLOOOO\n",
      "After epoch 8 train loss is 391.7504 valid loss is 21820.4198\n",
      "HELLLOOOO\n",
      "After epoch 9 train loss is 410.1410 valid loss is 21913.0977\n",
      "HELLLOOOO\n",
      "After epoch 10 train loss is 282.3038 valid loss is 23204.2985\n",
      "HELLLOOOO\n",
      "After epoch 11 train loss is 155.1605 valid loss is 24899.2778\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "sess.run(init)\n",
    "for i in range(EPOCHS):\n",
    "    for idx in range(0, train_anchor.shape[0], BATCH_SIZE):\n",
    "        anchor_batch, positive_batch, negative_batch = train_anchor[idx : idx + BATCH_SIZE], train_positive[idx : idx + BATCH_SIZE], train_negative[idx : idx + BATCH_SIZE]\n",
    "        sess.run(train, feed_dict = {caption_placeholder : anchor_batch, \\\n",
    "                                     frame_1_placeholder : positive_batch, \\\n",
    "                                     frame_2_placeholder : negative_batch})\n",
    "    \n",
    "    if i % 1 == 0:\n",
    "        print(\"HELLLOOOO\")\n",
    "        tot_loss, val_tot_loss = 0, 0\n",
    "        for idx in range(0, train_anchor.shape[0], BATCH_SIZE):\n",
    "            anchor_batch, positive_batch, negative_batch = train_anchor[idx : idx + BATCH_SIZE], train_positive[idx : idx + BATCH_SIZE], train_negative[idx : idx + BATCH_SIZE]\n",
    "            loss_ = sess.run(loss, feed_dict = {caption_placeholder : anchor_batch, \\\n",
    "                                            frame_1_placeholder : positive_batch, \\\n",
    "                                            frame_2_placeholder : negative_batch})\n",
    "            tot_loss += loss_\n",
    "            \n",
    "        for idx in range(0, val_anchor.shape[0], BATCH_SIZE):\n",
    "            val_anchor_batch, val_positive_batch, val_negative_batch = val_anchor[idx : idx + BATCH_SIZE], val_positive[idx : idx + BATCH_SIZE], val_negative[idx : idx + BATCH_SIZE]\n",
    "            val_loss_ = sess.run(loss, feed_dict = {caption_placeholder : val_anchor_batch, \\\n",
    "                                            frame_1_placeholder : val_positive_batch, \\\n",
    "                                            frame_2_placeholder : val_negative_batch})\n",
    "            val_tot_loss += val_loss_\n",
    "        print(\"After epoch {} train loss is {:.4f} valid loss is {:.4f}\".format(i, tot_loss, val_tot_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(461750, 50, 300)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_anchor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_triplets_variable'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, 'model_triplets_variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'/home/login/Paired/val_pair.pickle', 'rb') as f:\n",
    "    test_pairs_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the pad pairs function defined in make_triples\n",
    "def pad_pairs(pair_list):\n",
    "    # max_frame_step = max([each[1].shape[0] for each in pair_list])\n",
    "    max_frame_step = 50\n",
    "    # print(max_frame_step)\n",
    "    for idx, each in enumerate(pair_list):\n",
    "        pair_list[idx][1] = np.vstack([each[1], np.zeros((max_frame_step - each[1].shape[0], 500))])\n",
    "\n",
    "    # print([each[1].shape for each in pair_list])\n",
    "    # print([t.shape for t in temp])\n",
    "\n",
    "    # max_caption_step = max([each[2].shape[0] for each in pair_list])\n",
    "    max_caption_step = 50\n",
    "    # print(max_caption_step)\n",
    "\n",
    "    for idx, each in enumerate(pair_list):\n",
    "        if each[2].shape[0] < 50:\n",
    "            pair_list[idx][2] = np.vstack([each[2], np.zeros((max_caption_step - each[2].shape[0], 300))])\n",
    "        else:\n",
    "            pair_list[idx][2] = each[2][:50, :]\n",
    "\n",
    "    # print([each[2].shape for each in pair_list])\n",
    "    return pair_list\n",
    "\n",
    "test_pairs_list = pad_pairs(test_pairs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frames = np.stack([each[1] for each in test_pairs_list], axis = 0)\n",
    "test_captions = np.stack([each[2] for each in test_pairs_list], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(771, 50, 300)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_captions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = test_frames.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_latent_vector = sess.run(frame_out_1, feed_dict = {frame_1_placeholder : test_frames})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_latent_vector = sess.run(caption_out, feed_dict = {caption_placeholder : test_captions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(771, 256)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_latent_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives the rank of the corresponding clip for the given caption\n",
    "def get_rank(inp, idx):\n",
    "    out = [0] * len(inp)\n",
    "    for i, x in enumerate(sorted(range(len(inp)), key=lambda y: inp[y])):\n",
    "        out[x] = i\n",
    "    return out[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentile metric\n",
    "percentile_list = []\n",
    "for i, caption_ in enumerate(caption_latent_vector):\n",
    "    euclidean_norm = np.linalg.norm(frame_latent_vector - caption_, axis=1).tolist() # finding the closest clip to the caption\n",
    "    percentile = ((test_size - get_rank(euclidean_norm, i))/test_size) * 100 # finding the percentile of the closest\n",
    "    percentile_list.append(percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the top 20\n",
    "top20 = np.mean([1 if each > 80 else 0 for each in percentile_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of caption with atleast 20 percentile is 0.8416\n"
     ]
    }
   ],
   "source": [
    "print('The percentage of caption with atleast 20 percentile is {:.4f}'.format(top20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the top 10\n",
    "top10 = np.mean([1 if each > 90 else 0 for each in percentile_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of caption with atleast 10 percentile is is 0.5123\n"
     ]
    }
   ],
   "source": [
    "print('The percentage of caption with atleast 10 percentile is is {:.4f}'.format(top10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
