# ClipNet
- Agastya Teja Anumanchi, Gopal Seshadri, Vijaya Krishna Gopalakrishnan Partha

Our project ClipNet is aimed to provide a clip search platform where the system presents a subset of clips from a larger number of videos when presented with a caption. To achieve this we trained two recurrent neural network architectures in a Siamese network configuration. One network learns the frame embeddings extracted from the videos in ActivityNet and one network learns the word embeddings extracted from the captions corresponding to the activity in the videos previously mentioned. Both the frame embeddings and the caption embeddings captures a different of latent representations so we use Siamese network to find a latent space where this could be mapped. We tried a pair wise approach that learns a latent space based on the cosine similarity between the pairs of caption and frame embeddings, and a triplet based mining approach that involves the caption as the anchor and itâ€™s corresponding frame embeddings as positive and some other frame embeddings as negative.
